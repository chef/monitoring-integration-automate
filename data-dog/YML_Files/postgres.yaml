init_config:

instances:

  - host: localhost
  # change the "host" with actual URL for AWS managed postgres setup of Automate HA

    port: <To be filled>
    username: <To be filled>
    password: <To be filled>
    dbname: <To be filled>
    dbname: chef_authz_service
    dbname: chef_cereal_service
    dbname: dex
    dbname: chef_authn_service
    dbname: chef_teams_service
    dbname: secrets_service
    dbname: chef_applications_service
    dbname: notifications_service
    dbname: nodemanager_service
    dbname: chef_license_control_service
    dbname: chef_compliance_service
    dbname: chef_session_service
    dbname: chef_config_mgmt_service
    dbname: chef_ingest_service
    dbname: chef_infra_proxy
    dbname: data_feed_service
    dbname: automate-cs-bookshelf
    dbname: automate-cs-oc-bifrost
    dbname: automate-cs-oc-erchef

    ## @param dbstrict - boolean - optional - default: false
    ## Whether to restrict the scope of the check to just the database in question.
    ## Set to `true` if you only want to gather metrics from the database provided in the dbname parameter.
    #
    dbstrict: true
    ssl: 'prefer'

    # query_timeout: 1000

    ## @param relations - (list of string or mapping) - optional
    ## The list of relations/tables must be specified here to track per-relation (table) metrics.
    ## If enabled, `dbname` should be specified to collect database-specific relations metrics.
    ## You can either specify a single relation by its exact name in 'relation_name' or use a regex to track metrics
    ## from all matching relations (useful in cases where relation names are dynamically generated, e.g. TimescaleDB).
    ## Each relation generates many metrics (10 + 10 per index).
    ## By default all schemas are included. To track relations from specific schemas only,
    ## you can specify the `schemas` attribute and provide a list of schemas to use for filtering.
    ##
    ## Note: For compatibility reasons you can also use the following syntax to track relations metrics by specifying
    ## the list of table names. All schemas are included and regex are not supported.
    ## relations:
    ##   - <TABLE_NAME1>
    ##   - <TABLE_NAME2>
    #
    # relations:
    #   - relation_name: <TABLE_NAME>
    #     schemas:
    #     - <SCHEMA_NAME1>
    #   - relation_regex: <TABLE_PATTERN>

    ## @param max_relations - integer - optional - default: 300
    ## Determines the maximum number of relations to fetch.
    #
    # max_relations: 300

    ## @param collect_function_metrics - boolean - optional - default: false
    ## If set to true, collects metrics regarding PL/pgSQL functions from pg_stat_user_functions.
    #
    collect_function_metrics: true

    ## @param collect_count_metrics - boolean - optional - default: true
    ## Collect count of user tables up to max_relations size from pg_stat_user_tables.
    #
    collect_count_metrics: true

    ## @param collect_activity_metrics - boolean - optional - default: false
    ## Collect metrics regarding transactions from pg_stat_activity. Please make sure the user
    ## has sufficient privileges to read from pg_stat_activity before enabling this option.
    #
    collect_activity_metrics: true

    ## @param collect_database_size_metrics - boolean - optional - default: true
    ## Collect database size metrics.
    #
    collect_database_size_metrics: true

    ## @param collect_default_database - boolean - optional - default: false
    ## Include statistics from the default database 'postgres' in the check metrics.
    #
    collect_default_database: true

    ## @param tag_replication_role - boolean - optional - default: false
    ## Tag metrics and checks with `replication_role:<master|standby>`.
    #
    tag_replication_role: true

    ## @param table_count_limit - integer - optional - default: 200
    ## The maximum number of tables to collect metrics from.
    #
    table_count_limit: 10000

    ## @param custom_queries - list of mappings - optional
    ## Define custom queries to collect custom metrics from your PostgreSQL
    ## See Datadog FAQ article for a guide on collecting custom metrics from PostgreSQL:
    ## https://docs.datadoghq.com/integrations/faq/postgres-custom-metric-collection-explained/
    #
    # custom_queries:
    #   - metric_prefix: postgresql
    #     query: <QUERY>
    #     columns:
    #     - name: <COLUNMS_1_NAME>
    #       type: <COLUMNS_1_TYPE>
    #     - name: <COLUNMS_2_NAME>
    #       type: <COLUMNS_2_TYPE>
    #     tags:
    #     - <TAG_KEY>:<TAG_VALUE>
    custom_queries:
      - metric_prefix: postgresql
        query: SELECT application_name, usename, COUNT(*) FROM pg_stat_activity WHERE application_name NOT LIKE 'psql' AND (application_name <> '') IS TRUE GROUP BY application_name, usename
        columns:
        - name: application_name
          type: tag
        - name: pg_user
          type: tag
        - name: connected_applications
          type: gauge
        tags:
          - metric-type:custom

    ## @param application_name - string - optional - default: datadog-agent
    ## The application_name can be any string of less than NAMEDATALEN characters (64 characters in a standard build).
    ## It is typically set by an application upon connection to the server.
    ## The name is displayed in the pg_stat_activity view and included in CSV log entries.
    #
    application_name: datadog-agent

    ## @param deep_database_monitoring - boolean - optional - default: false
    ## Set to `true` to enable the ALPHA features for Deep Database Monitoring.
    ##
    ## If you would like to hear more about Deep Database Monitoring, please reach out to your customer
    ## success manager or Datadog support.
    #
    deep_database_monitoring: true

    ## @param statement_metrics_limits - mapping - optional - default: false
    ## Defines the top and bottom limits on queries to track for each metric. These limits apply only to the
    ## ALPHA features of Deep Database Monitoring. It is recommended to leave these settings at their default
    ## values unless instructed otherwise. This API may change in the future.
    ##
    ## If you would like to hear more about Deep Database Monitoring, please reach out to your customer
    ## success manager or Datadog support.
    #
    # statement_metrics_limits:
    #   calls:
    #   - 100
    #   - 0
    #   time:
    #   - 100
    #   - 0
    #   <METRIC_COLUMN>:
    #   - <TOP_K_LIMIT>
    #   - <BOTTOM_K_LIMIT>

    ## @param pg_stat_statements_view - string - optional - default: show_pg_stat_statements()
    ## Set this value if you want to define a custom view or function to allow the datadog user to query the
    ## `pg_stat_statements` table, which is useful for restricting the permissions given to the datadog agent.
    ## Please note this is an ALPHA feature and is subject to change or deprecation without notice.
    #
    pg_stat_statements_view: show_pg_stat_statements()

    statement_samples:

        ## @param enabled - boolean - optional - default: false
        ## Enable collection of statement samples. Requires `deep_database_monitoring: true`.
        #
        enabled: true

        ## @param collections_per_second - number - optional - default: 1
        ## Set the maximum statement sample collection rate. Each collection involves a single query to
        ## `pg_stat_activity` followed by at most one `EXPLAIN` query per unique statement seen.
        #
        # collections_per_second: 1

        ## @param explain_function - string - optional - default: datadog.explain_statement
        ## Override the default function used to collect execution plans for statements.
        #
        # explain_function: datadog.explain_statement

        ## @param explained_statements_per_hour_per_query - integer - optional - default: 60
        ## Set the rate limit for how many execution plans will be collected per hour per normalized statement.
        #
        # explained_statements_per_hour_per_query: 60

        ## @param samples_per_hour_per_query - integer - optional - default: 15
        ## Set the rate limit for how many statement sample events will be ingested per hour per normalized execution
        ## plan.
        #
        # samples_per_hour_per_query: 15

        ## @param explained_statements_cache_maxsize - integer - optional - default: 5000
        ## Set the max size of the cache used for the explained_statements_per_hour_per_query rate limit. This should
        ## be increased for databases with a very large number unique normalized queries which exceed the cache's
        ## limit.
        #
        # explained_statements_cache_maxsize: 5000

        ## @param seen_samples_cache_maxsize - integer - optional - default: 10000
        ## Set the max size of the cache used for the samples_per_hour_per_query rate limit. This should be increased
        ## for databases with a very large number of unique normalized execution plans which exceed the cache's limit.
        #
        # seen_samples_cache_maxsize: 10000

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>
    tags:
      - "env:a2ha"
      - "customer:a2ha_customer1"
    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

## Log Section
##
## type - required - Type of log input source (tcp / udp / file / windows_event)
## port / path / channel_path - required - Set port if type is tcp or udp.
##                                         Set path if type is file.
##                                         Set channel_path if type is windows_event.
## source  - required - Attribute that defines which Integration sent the logs.
## encoding - optional - For file specifies the file encoding, default is utf-8, other
##                       possible values are utf-16-le and utf-16-be.
## service - optional - The name of the service that generates the log.
##                      Overrides any `service` defined in the `init_config` section.
## tags - optional - Add tags to the collected logs.
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - type: file
#     path: /path/to/postgres.log
#     source: postgresql
#     service: <SERVICE>
#     log_processing_rules:
#     - type: multi_line
#       pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
#       name: new_log_start_with_date
